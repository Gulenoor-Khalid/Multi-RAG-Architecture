# ğŸ–¼ï¸ HÆ°á»›ng dáº«n sá»­ dá»¥ng BLIP vá»›i RAG

## Tá»•ng quan
Há»‡ thá»‘ng RAG Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p BLIP (Bootstrapping Language-Image Pre-training) Ä‘á»ƒ há»— trá»£ chat vá»›i cáº£ **text vÃ  image**.

## TÃ­nh nÄƒng má»›i

### 1. **Visual Question Answering (VQA)**
- Äáº·t cÃ¢u há»i vá» ná»™i dung hÃ¬nh áº£nh
- BLIP sáº½ phÃ¢n tÃ­ch vÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn hÃ¬nh áº£nh

### 2. **Image Captioning**
- Tá»± Ä‘á»™ng táº¡o mÃ´ táº£ cho hÃ¬nh áº£nh
- CÃ³ thá»ƒ thÃªm Ä‘iá»u kiá»‡n (conditional text) Ä‘á»ƒ hÆ°á»›ng dáº«n viá»‡c táº¡o caption

### 3. **Multimodal RAG**
- Káº¿t há»£p thÃ´ng tin tá»« documents (text) vÃ  images
- Truy váº¥n Ä‘á»“ng thá»i cáº£ text vÃ  image context

## CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t dependencies má»›i
```bash
pip install -r requirements.txt
```

CÃ¡c thÆ° viá»‡n má»›i Ä‘Æ°á»£c thÃªm:
- `Pillow==10.4.0` - Xá»­ lÃ½ hÃ¬nh áº£nh

### 2. Khá»Ÿi Ä‘á»™ng láº¡i backend
```bash
# Windows
start.bat

# hoáº·c
python -m uvicorn backend.app.main:app --reload --host 0.0.0.0 --port 8000
```

## CÃ¡ch sá»­ dá»¥ng

### 1. Upload Image qua UI

1. Má»Ÿ giao diá»‡n web: `http://localhost:3000`
2. Trong sidebar, tÃ¬m pháº§n **"ğŸ–¼ï¸ Upload Image"**
3. Chá»n file hÃ¬nh áº£nh (PNG, JPG, JPEG, etc.)
4. Click **"Upload Image"**
5. Chá»n mode:
   - **Q&A vá» áº£nh**: Äá»ƒ Ä‘áº·t cÃ¢u há»i vá» hÃ¬nh áº£nh
   - **MÃ´ táº£ áº£nh**: Äá»ƒ táº¡o mÃ´ táº£ cho hÃ¬nh áº£nh

### 2. Chat vá»›i Image

Sau khi upload image:

**Mode Q&A:**
```
User: "What is in this image?"
Bot: [PhÃ¢n tÃ­ch hÃ¬nh áº£nh vÃ  tráº£ lá»i]

User: "What color is the car?"
Bot: [Tráº£ lá»i dá»±a trÃªn hÃ¬nh áº£nh]
```

**Mode MÃ´ táº£:**
```
User: "Describe this image"
Bot: [Táº¡o mÃ´ táº£ chi tiáº¿t cho hÃ¬nh áº£nh]

User: "A photo of"
Bot: [Táº¡o caption báº¯t Ä‘áº§u vá»›i "A photo of..."]
```

### 3. Káº¿t há»£p RAG + Image

Báº¡n cÃ³ thá»ƒ:
1. Upload documents (PDF, TXT, DOCX)
2. Upload image
3. Äáº·t cÃ¢u há»i â†’ Há»‡ thá»‘ng sáº½ tÃ¬m kiáº¿m cáº£ trong documents VÃ€ phÃ¢n tÃ­ch image Ä‘á»ƒ tráº£ lá»i

**VÃ­ dá»¥:**
```
Documents: TÃ i liá»‡u vá» xe hÆ¡i
Image: HÃ¬nh áº£nh má»™t chiáº¿c xe
Query: "Chiáº¿c xe trong áº£nh thuá»™c loáº¡i nÃ o? NÃ³ cÃ³ nhá»¯ng tÃ­nh nÄƒng gÃ¬?"

â†’ Bot sáº½ káº¿t há»£p thÃ´ng tin tá»«:
  - PhÃ¢n tÃ­ch hÃ¬nh áº£nh (BLIP)
  - TÃ¬m kiáº¿m trong documents (RAG)
```

## API Endpoints

### 1. Upload Image
```http
POST /upload-image
Content-Type: multipart/form-data

file: [image file]
```

**Response:**
```json
{
  "message": "Image uploaded successfully",
  "filename": "image.jpg",
  "caption": "a photo of a red car"
}
```

### 2. Query vá»›i Image
```http
POST /query
Content-Type: application/json

{
  "query": "What is in this image?",
  "use_rag": true,
  "image_base64": "data:image/jpeg;base64,/9j/4AAQ...",
  "image_mode": "vqa",
  "max_tokens": 512,
  "temperature": 0.7
}
```

**Parameters:**
- `query`: CÃ¢u há»i/yÃªu cáº§u
- `use_rag`: CÃ³ sá»­ dá»¥ng RAG khÃ´ng
- `image_base64`: HÃ¬nh áº£nh dáº¡ng base64 (optional)
- `image_mode`: `"vqa"` hoáº·c `"caption"`

**Response:**
```json
{
  "answer": "The image shows a red sports car...",
  "model_used": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "sources": ["document1.pdf", "document2.txt"]
}
```

## Models BLIP

Há»‡ thá»‘ng sá»­ dá»¥ng 2 models BLIP:

### 1. BLIP Caption Model
- **Model:** `Salesforce/blip-image-captioning-base`
- **Chá»©c nÄƒng:** Táº¡o mÃ´ táº£ cho hÃ¬nh áº£nh
- **KÃ­ch thÆ°á»›c:** ~990MB
- **VRAM:** ~2-3GB

### 2. BLIP VQA Model
- **Model:** `Salesforce/blip-vqa-base`
- **Chá»©c nÄƒng:** Tráº£ lá»i cÃ¢u há»i vá» hÃ¬nh áº£nh
- **KÃ­ch thÆ°á»›c:** ~990MB
- **VRAM:** ~2-3GB

## Cáº¥u hÃ¬nh nÃ¢ng cao

### Environment Variables

ThÃªm vÃ o file `.env`:

```bash
# BLIP Models
BLIP_CAPTION_MODEL=Salesforce/blip-image-captioning-base
BLIP_VQA_MODEL=Salesforce/blip-vqa-base
```

### TÃ¹y chá»‰nh trong code

**Load model:**
```python
from backend.app.models.blip_processor import BLIPImageProcessor

blip = BLIPImageProcessor()
await blip.load_caption_model()
await blip.load_vqa_model()
```

**Generate caption:**
```python
from PIL import Image

image = Image.open("image.jpg")
caption = await blip.generate_caption(image)
print(caption)  # "a photo of a red car"
```

**Answer question:**
```python
answer = await blip.answer_question(image, "What color is the car?")
print(answer)  # "red"
```

## Performance Tips

### 1. Memory Management
- Chá»‰ load model khi cáº§n thiáº¿t
- DÃ¹ng `blip.unload_models()` Ä‘á»ƒ giáº£i phÃ³ng VRAM

### 2. Batch Processing
```python
images = [image1, image2, image3]
captions = await blip.batch_generate_captions(images)
```

### 3. GPU vs CPU
- **GPU (CUDA):** Nhanh hÆ¡n ~10x, cáº§n 2-3GB VRAM
- **CPU:** Cháº­m hÆ¡n nhÆ°ng khÃ´ng cáº§n GPU

## Troubleshooting

### 1. Out of Memory
```
Error: CUDA out of memory
```
**Giáº£i phÃ¡p:**
- Giáº£m batch size
- Unload models khÃ´ng dÃ¹ng
- DÃ¹ng CPU thay vÃ¬ GPU

### 2. Model load cháº­m
**Láº§n Ä‘áº§u:** Download model (~1-2GB), máº¥t 5-10 phÃºt
**Láº§n sau:** Load tá»« cache, nhanh hÆ¡n

### 3. Lá»—i PIL/Image
```bash
pip install --upgrade Pillow
```

## Examples

### Example 1: Basic VQA
```python
from PIL import Image
from backend.app.models.blip_processor import BLIPImageProcessor

blip = BLIPImageProcessor()
image = Image.open("car.jpg")

answer = await blip.answer_question(image, "What is this?")
print(answer)  # "car"
```

### Example 2: Conditional Captioning
```python
caption = await blip.generate_caption(image, "a photo of")
print(caption)  # "a photo of a red sports car on the road"
```

### Example 3: Multimodal RAG
```python
# User uploads image + documents
# User asks: "What type of car is this and what are its specs?"
# System:
# 1. Analyzes image â†’ "red sports car"
# 2. Searches documents â†’ "Ferrari specs..."
# 3. Combines â†’ Complete answer
```

## Architecture

```
User Query + Image
     â†“
Frontend (image upload)
     â†“
Backend API (/query)
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BLIP Processor â”‚   RAG Engine     â”‚
â”‚  (VQA/Caption)  â”‚  (Vector Search) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“                    â†“
  Image Context      Text Context
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
         LLM Manager
              â†“
          Response
```

## Roadmap

- [x] BLIP integration
- [x] VQA support
- [x] Image captioning
- [x] Multimodal RAG
- [ ] Multiple image support
- [ ] Image-to-image search
- [ ] OCR integration
- [ ] Video frame analysis

## Credits

- **BLIP:** Salesforce Research
- **Models:** Hugging Face Transformers
- **Framework:** FastAPI, LangChain

---

**Enjoy multimodal RAG! ğŸ‰**
